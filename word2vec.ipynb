{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"word2vec.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1JjyJ3m03nhe3amsWMwb8veIVidZhYNBw","authorship_tag":"ABX9TyPKF+/WI7cAie9VTbOA+iZF"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"-D2TiERF2-xc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623282063665,"user_tz":-60,"elapsed":292,"user":{"displayName":"Fiona Chan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjnM99lJbufIGVDkXXe0V7wymgLNDyAvbQIMqN4=s64","userId":"04069082546873329207"}},"outputId":"1b8fc108-a664-430f-93eb-fd9e10c47f25"},"source":["import gensim\n","import pickle\n","import re\n","import nltk\n","import pandas as pd\n","nltk.download('punkt')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"HG4igIc63bq1","executionInfo":{"status":"ok","timestamp":1623282066390,"user_tz":-60,"elapsed":425,"user":{"displayName":"Fiona Chan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjnM99lJbufIGVDkXXe0V7wymgLNDyAvbQIMqN4=s64","userId":"04069082546873329207"}}},"source":["dataset_marker = \"S09_question_answer_pairs\""],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"j5pwbRfbN4a9","executionInfo":{"status":"ok","timestamp":1623282068651,"user_tz":-60,"elapsed":827,"user":{"displayName":"Fiona Chan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjnM99lJbufIGVDkXXe0V7wymgLNDyAvbQIMqN4=s64","userId":"04069082546873329207"}}},"source":["df = pd.read_csv('S09_question_answer_pairs.txt', sep='\\t')\n","data = df[\"Question\"].append(df[\"Answer\"])"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"i7j-i1m-3jTg","executionInfo":{"status":"ok","timestamp":1623282084928,"user_tz":-60,"elapsed":1888,"user":{"displayName":"Fiona Chan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjnM99lJbufIGVDkXXe0V7wymgLNDyAvbQIMqN4=s64","userId":"04069082546873329207"}}},"source":["class SentenceGenerator(object):\n","    def __iter__(self):\n","        with open(\"S09_question_answer_pairs.txt\") as file_in:\n","          for indx,line in enumerate(file_in):\n","            if indx > 0:\n","              data = line.split(\"\\t\")\n","              data = data[1]+data[2]\n","              data = data.strip().lower().replace(\"<sssss> \", \"\")\n","              data = re.sub(r\"[^A-Za-z0-9()<>:;,.!?\\'\\\"]\", \" \", data)\n","              data = re.sub(r\",\", \" , \", data)\n","              data = re.sub(r\"!\", \" ! \", data)\n","              data = re.sub(r\"\\.\", \" . \", data)\n","              data = re.sub(r\"\\?\", \" ? \", data)\n","              data = nltk.word_tokenize(data)\n","              yield data\n"," \n","model = gensim.models.Word2Vec(sentences=SentenceGenerator(), size=128, min_count=5)\n","\n","word_embeddings = dict()\n","\n","for word in model.wv.vocab:\n","    word_embeddings[word] = model.wv[word]\n","with open(\"input/%s_pretrained_embeddings.dat\" % dataset_marker, \"wb+\") as file_out:\n","    pickle.dump(word_embeddings, file_out)"],"execution_count":7,"outputs":[]}]}